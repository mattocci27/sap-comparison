@unpublished{Vehtari2014,
  title = {Bayesian Leave-One-out Cross-Validation Approximations for {{Gaussian}} Latent Variable Models},
  author = {Vehtari, Aki and Mononen, Tommi and Tolvanen, Ville and Sivula, Tuomas and Winther, Ole},
  date = {2014-12},
  eprint = {1408.4050v2},
  eprinttype = {arxiv},
  issn = {15337928},
  url = {http://arxiv.org/abs/1412.7461},
  abstract = {The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods.},
  archiveprefix = {arXiv},
  isbn = {1412.7461},
  file = {/Users/mattocci/Dropbox/zotero_papers/arXiv preprint arXiv1408.4050v2/2014/Vehtari et al_2014_arXiv preprint arXiv1408.4050v2.pdf}
}

@article{Vehtari2017,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  date = {2017},
  journaltitle = {Statistics and Computing},
  number = {27},
  eprint = {1507.04544},
  eprinttype = {arxiv},
  pages = {1413--1432},
  publisher = {{Springer US}},
  issn = {15731375},
  doi = {10.1007/s11222-016-9696-4},
  url = {http://link.springer.com/10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  isbn = {1507.04544},
  keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
  file = {/Users/mattocci/Dropbox/zotero_papers/Statistics and Computing/2016/Vehtari et al_2016_Statistics and Computing.pdf;/Users/mattocci/Dropbox/zotero_papers/Statistics and Computing/2016/Vehtari et al_2016_Statistics and Computing.pdf}
}

@article{Vehtari2021,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved Rˆ}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
  date = {2021-06},
  journaltitle = {Bayesian Analysis},
  shortjournal = {ba},
  volume = {16},
  number = {2},
  pages = {667--718},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full},
  urldate = {2023-01-15},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic Rˆ of Gelman and Rubin (1992) has serious flaws. Traditional Rˆ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/Users/mattocci/Dropbox/zotero_papers/Vehtari et al_2021_Rank-Normalization, Folding, and Localization.pdf}
}

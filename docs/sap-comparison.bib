@unpublished{Vehtari2014,
  title = {Bayesian Leave-One-out Cross-Validation Approximations for {{Gaussian}} Latent Variable Models},
  author = {Vehtari, Aki and Mononen, Tommi and Tolvanen, Ville and Sivula, Tuomas and Winther, Ole},
  date = {2014-12},
  eprint = {1408.4050v2},
  eprinttype = {arxiv},
  issn = {15337928},
  url = {http://arxiv.org/abs/1412.7461},
  abstract = {The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods.},
  archiveprefix = {arXiv},
  isbn = {1412.7461},
  file = {/Users/mattocci/Dropbox/zotero_papers/arXiv preprint arXiv1408.4050v2/2014/Vehtari et al_2014_arXiv preprint arXiv1408.4050v2.pdf}
}

@article{Vehtari2017,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  date = {2017},
  journaltitle = {Statistics and Computing},
  number = {27},
  eprint = {1507.04544},
  eprinttype = {arxiv},
  pages = {1413--1432},
  publisher = {{Springer US}},
  issn = {15731375},
  doi = {10.1007/s11222-016-9696-4},
  url = {http://link.springer.com/10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  isbn = {1507.04544},
  keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
  file = {/Users/mattocci/Dropbox/zotero_papers/Statistics and Computing/2016/Vehtari et al_2016_Statistics and Computing.pdf;/Users/mattocci/Dropbox/zotero_papers/Statistics and Computing/2016/Vehtari et al_2016_Statistics and Computing.pdf}
}

@article{Vehtari2021,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved Rˆ}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
  date = {2021-06},
  journaltitle = {Bayesian Analysis},
  shortjournal = {ba},
  volume = {16},
  number = {2},
  pages = {667--718},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full},
  urldate = {2023-01-15},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic Rˆ of Gelman and Rubin (1992) has serious flaws. Traditional Rˆ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/Users/mattocci/Dropbox/zotero_papers/Vehtari et al_2021_Rank-Normalization, Folding, and Localization.pdf}
}

@article{Stekhoven2012,
  title = {{{MissForest}}—Non-Parametric Missing Value Imputation for Mixed-Type Data},
  author = {Stekhoven, Daniel J. and Bühlmann, Peter},
  date = {2012-01-01},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  volume = {28},
  number = {1},
  pages = {112--118},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btr597},
  url = {https://doi.org/10.1093/bioinformatics/btr597},
  urldate = {2023-06-30}
}

@article{Warton2006,
  title = {Bivariate Line-Fitting Methods for Allometry},
  author = {Warton, David I. and Wright, Ian J. and Falster, Daniel S. and Westoby, Mark},
  year = {2006},
  journal = {Biological Reviews of the Cambridge Philosophical Society},
  volume = {81},
  number = {2},
  pages = {259--291},
  publisher = {{Wiley Online Library}},
  issn = {14647931},
  doi = {10.1017/S1464793106007007},
  abstract = {Warton DI, Wright IJ, Falster DS, Westoby M (2006) Bivariate line-fitting methods for allometry. Biol Rev 81: 259\textendash 291. (citation from Arnold and Zink 2011)},
  isbn = {1464-7931 (Print)\textbackslash n0006-3231 (Linking)},
  pmid = {16573844},
  keywords = {Analysis of covariance,Errors-in-variables models,Functional and structural relationships,Measurement error,Method-of-moments regression,Model II regression,Standardised major axis,Test for common slopes}
}

@article{Carpenter2017,
  title = {Stan : {{A Probabilistic Programming Language}}},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  date = {2017-01-11},
  journaltitle = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  pages = {1--32},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  url = {http://www.jstatsoft.org/v76/i01/},
  urldate = {2018-01-22},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
  keywords = {\#nosource}
}

@article{Gelman2008,
  title = {A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models},
  author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu Sung},
  date = {2008},
  journaltitle = {Annals of Applied Statistics},
  volume = {2},
  number = {4},
  eprint = {0901.4011},
  eprinttype = {arxiv},
  pages = {1360--1383},
  issn = {19326157},
  doi = {10.1214/08-AOAS191},
  url = {http://dx.doi.org/10.1214/08-aoas191},
  abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-\$t\$ prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-\$t\$ prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
  archiveprefix = {arXiv},
  isbn = {19326157},
  keywords = {Bayesian inference,Generalized linear model,Hierarchical model,Least squares,Linear regression,Logistic regression,Multilevel model,Noninformative prior distribution,Weakly informative prior distribution}
}

@book{Gelman2013,
  title = {Bayesian {{Data Analysis}}, {{Third Edition}}},
  author = {Gelman, A and Carlin, J B and Stern, H S and Dunson, D B and Vehtari, A and Rubin, D B},
  date = {2013},
  publisher = {{Chapman \& Hall/CRC}},
  location = {{Boca Raton, FL, USA.}},
  url = {https://books.google.com.pa/books?id=eSHSBQAAQBAJ},
  isbn = {978-1-4398-9820-8},
  pagetotal = {675},
  keywords = {\#nosource}
}

@article{Landau2021,
  title = {The Targets {{R}} Package: A Dynamic {{Make-like}} Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing},
  shorttitle = {The Targets {{R}} Package},
  author = {Landau, William Michael},
  date = {2021-01-15},
  journaltitle = {Journal of Open Source Software},
  volume = {6},
  number = {57},
  pages = {2959},
  issn = {2475-9066},
  doi = {10.21105/joss.02959},
  url = {https://joss.theoj.org/papers/10.21105/joss.02959},
  urldate = {2022-06-19},
  abstract = {Landau, W. M., (2021). The targets R package: a dynamic Make-like function-oriented pipeline toolkit for reproducibility and high-performance computing. Journal of Open Source Software, 6(57), 2959, https://doi.org/10.21105/joss.02959},
  langid = {english}
}

@manual{RCoreTeam2022,
  type = {manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2022},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

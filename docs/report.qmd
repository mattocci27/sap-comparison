---
title: "Figures"
author: "Masatoshi Katabuchi"
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 11pt
csl: apa.csl
bibliography: [sap-comparison.bib]
crossref:
  fig-title: Fig.
  fig-prefix: Fig.
format:
  html:
    theme: spacelab #readable #sandstone #spacelab #flatly
    toc: true
    toc-depth: 2
    toc-title: Contents
    embed-resources: true
    smooth-scroll: true
    highlight-style: github
---

```{r global_options, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  cache = FALSE,
  fig.align = "center",
  fig.show = "hold",
  root.dir = rprojroot::find_root('_targets.R')
)
```

```{r,include=FALSE}
library(tidyverse)
library(tarchetypes)
library(here)
library(kableExtra)
library(targets)
library(loo)
library(smatr)
library(GGally)
library(ggridges)
library(ggrepel)
library(bayesplot)
source(here("R", "stan.R"))
source(here("R", "render.R"))
source(here("R", "figs.R"))
anova_yml <- yaml::yaml.load_file(here("yml/anova.yml"))
para <- read_csv(here("data/ab_var_clean_008.csv"))
```

**
Note:
Method parts are tentative.
**

```{r, include=FALSE}
# s_fig_num("coef_int_sd")
# s_fig_num("coef_int_mean")
# s_fig_num("coef_int_diff")
s_fig_num("diagram")
s_fig_num("cross_section")
s_fig_num("coef_intervals_pres_tens")
s_fig_num("coef_int_quad_mean")
s_fig_num("pool_multi")
s_fig_num("ab_points")
s_fig_num("pg_multi")
s_fig_num("pg_ribbon_a")
s_fig_num("pg_ribbon_b")
s_fig_num("traits_points_si")
s_table("summary_of_published_tdm_coefficients")
```

# Not in this file

- `r s_fig("diagram")`: diagram
- `r s_fig("cross_section")`: cross_section
- `r s_table("summary_of_published_tdm_coefficients")`: summary of published tdm coefficients
- Soil and tree properties

# Pressure vs. tension induced flow

```{r, include=FALSE}
# tar_load(ks_spp_err_csv)
ks_spp_err_csv <- withr::with_dir(rprojroot::find_root('_targets.R'), targets::tar_read(ks_spp_err_csv))
d <- read_csv(here(ks_spp_err_csv))
sma_fit <- sma(log10(tens_calib_mean) ~ log10(pres_calib_mean), d)
```

```{r, include=FALSE}
sma_slope <- sma_fit$coef[[1]][2, ] |>
  round(2) |> format(nsmall = 2)
sma_int <- sma_fit$coef[[1]][1, ] |> round(2) |> format(nsmall = 2)
```

## Results

We found that pressure type (pressure vs. tension) has a marginal effect on the induced flow rate.
Although a strong relationship emerged within each species between tension- and pressure-derived *K~s~*, a significant shift was found in elevation between pressure- and tension-induced flow rates
(
`r fig("sma")`a,
SMA intercept:
`r sma_int[1]`
[95% CI:
`r sma_int[2]`
,
`r sma_int[3]`
]
).
The SMA slope was different from 1
(
`r fig("sma")`a,
SMA slope:
`r sma_slope[1]`
[95% CI:
`r sma_slope[2]`
,
`r sma_slope[3]`
]
).
Pressure-induced flow rates tend to be greater than tension-induced flow rates for samples of low flow rates, converging with increasing flow rates (`r fig("sma")`a).

Pressure-level (i.e., 0.02, 0.05, 0.08 MPa) explained very little variation in the differences between pressure- and tension-induced flow rates,
whereas species difference explained much more variation
(`r str_split(anova_yml$pressure, " ")[[1]][1]` vs. `r str_split(anova_yml$species, " ")[[1]][1]` of the total variance,
Figs. `r fig_num("sma")`b &
`r s_fig_num("coef_intervals_pres_tens")`a;
`r s_table("anova_pres_tens")`).
After controlling species and pressure-level effects (i.e., hierarchical model), pressure induced-flow rates still likely to be greater than tension-induced flow rates (`r s_fig("coef_intervals_pres_tens")`b).

## `r fig("sma")`

Flow rate under three levels of driving force employed as pressure or tension, normalized to maximum hydraulic conductivity (Ks) to account for small differences in sample length and diameter demonstrating the effect of increasing force on Ks (??).
(A) Relationship between sample mean pressure- and tension-induced flow rates.
Blue solid line indicates a standardized major axis (SMA) regression.
The 95% confidence interval is represented as the shaded area.
(B) Comparisons between pressure- and tension-induced flow rates at three levels for three diffuse-porous, one ring-porous, and one liana species.
The center line in each box indicates the median, and upper and lower box sides indicate the interquartile range.
The whiskers extend to a maximum of 1.5 times the interquartile range.
This is only to simplify the visualization, and statistical results are shown in SX.
HH, *Hopea hongayensis*;
VM, *Vatica mangachapoi*;
HB, *Hevea brasiliensis*;
TG, *Tectona grandis*;
AP, *Acacia pennata*.


![sma_ks](`r here::here("figs/sma_ks.png")`)


### `r s_table("anova_pres_tens")`

Posterior medians and 50% credible intervals of the explained variance for the ratio between pressure- and tension-induced flow rates.

```{r, echo=FALSE}
tibble(Factor = names(anova_yml) |> str_to_title(),
  Posterior = unlist(anova_yml)) |>
  kbl() |>
  kable_classic(full_width = F)
```

## `r s_fig("coef_intervals_pres_tens")`

Posterior medians (circles), 50% (rectangles), and 95% (thin lines) Bayesian credible intervals (CIs) for species and pressure effects on the ratio between pressure and tension induced flows.
(A) Estimates for standard deviations.
(B) Estimates for the effect of each predictor.
(C) Estimates for differences in effects between predictors.
There is large variation but pressure-level (i.e., 0.02, 0.05, 0.08 MPa) explained very little variation in the differences between pressure- and tension-induced flow rates of the total variance in (A).
Positive values indicate pressure induced flow rates are greater than tension induced flow rates in (B) and (C).
The positive ks_ratio parameter in (B) suggests that pressure induced flow rates tend to be greater than tension induced flow rates.
The estimate for AP is smaller than HH, TG, and VM in (C), suggesting that the difference between pressure and tension induced flow rates tends to be smaller for AP compared to HH, TG, and VM.
HB shows the similar pattern with AP.
ks_ratio, ratio of pressure induced flow rates to tension induced flow rates;
HH, *Hopea hongayensis*;
VM, *Vatica mangachapoi*;
HB, *Hevea brasiliensis*;
TG, *Tectona grandis*;
AP, *Acacia pennata*;
p_2, 0.02 MPa;
p_5, 0.05 MPa;
p_8, 0.08 MPa.

![coef_intervals_mean](`r here::here("figs/coef_intervals_pres_tens.png")`){ width=650 }

## Methods

**This is just a note for now.**

### Analysis (ANOVA-like hierarchical model)

We can make the hierarchical model like the previous model, but we can include measurement errors from the raw data too.
We don't want to waste the data!

$$
y^{meas}_{1ijk} \sim N(y_{1ijk}, \sigma_{y1ijk})
$$

$$
y^{meas}_{2ijk} \sim N(y_{2ijk}, \sigma_{y2ijk})
$$

$$
\alpha_j \sim N(0, \sigma_{\alpha})
$$
$$
\beta_j \sim N(0, \sigma_{\beta})
$$

$$
\mathrm{ln}\; y_{1ijk} \sim N(\mu + \alpha_j + \beta_k + \mathrm{ln}\; y_{2ijk}, \sigma)
$$

- *i*: Observation

- *j*: Species

- *k*: Pressure level

- $y^{meas}_{1ijk}$: Measured mean pressure induced flow rates for *i*th observation from *j*th species, under *k*th pressure level (data).
$y^{meas}_{2ijk}$ is for tension calibration.

- $y_{1ijk}$: True pressure induced flow rates for *i*th observation from *j*th species, under *k*th pressure level (parameter).
$y_{2ijk}$ is for tension calibration.

- $\sigma_{y1jk}$: Standard deviation of pressure induced flow rates for *i*th obervation from *j*th species, under *k*th pressure level (data).
This data can be estimated from the raw data with multiple measurements.
$\sigma_{y2jk}$ is for tension calibration.

- $\mu$: Overall (species and pressure-level independent) effects.

- $\alpha_j$: Species effect

- $\beta_k$: Pressure-level effect

- $\sigma_{\alpha}$: Standard deviation of the species effect

- $\sigma_{\beta}$: Standard deviation of the pressure-level effect

- $\sigma$: Standard deviation of the model (i.e., residuals)


I will show several figures from this single analysis.

We can translate the posterior distribution of standard deviations into an ANOVA like statement.

For example, we can calculate the species effect like:

$$
\frac{\sigma^2_{\alpha}}{\sigma^2_{\alpha} + \sigma^2_{\beta} + \sigma^2} \times 100\;(\%)
$$

The effect of interactions was not selected after model selections.

note:

- median and 50% CIs

- Species: `r anova_yml$species`

- Pressure: `r anova_yml$pressure`

- Residuals: `r anova_yml$residuals`



# Pressure vs. number of active vessels

## Results

Overall, increasing pressure tended to linearly increase the proportion of vessels participating in flow (Figs. `r fig_num("count_pressure")` & `r s_fig_num("coef_int_quad_mean")`).
Although quadratic function best described the responses, the proportion of vessels participating in flow rate for *Hopea hongayensi* increased was highest in the intermediate pressure, but either remained the same or increased to a maximum moving from the intermediate to the highest pressure in the other four species (Figs. `r fig_num("count_pressure")` & `r s_fig_num("coef_int_quad_mean")`).


## `r fig("count_pressure")`

Changes in the proportion of silicon-stained vessels along perfusion pressures for diffuse-porous (A-C: red), ring-porous (D: green), and liana species (E: purple).
Lines and shaded areas indicate the posterior medians and 95% credible intervals, respectively.
The size of data points is proportional to the total number of vessels.
The right columns are fluorography images of active vessels under UV light using silicon-perfusion.
The red scale at the bottom right represents 500$\micro$m.

![count_pressure_quadratic](`r here::here("figs/count_pressure_quadratic.png")`){ width=200 }

## `r s_fig("coef_int_quad_mean")`

Posterior medians (circles), 50% (rectangle), and 95% (thin lines) credible intervals (CIs) for model coefficients of the effects of perfusion pressure on the probability of a vessel filled with silicon.
All, Common effects across all the species;
HH, *Hopea hongayensis*;
VM, *Vatica mangachapoi*;
HB, *Hevea brasiliensis*;
TG, *Tectona grandis*;
AP, *Acacia pennata*.

![coef_intervals_logistic](`r here::here("figs/coef_intervals_logistic.png")`){ width=450 }

## Methods

**tentative**

We modeled the probability of a vessel filled with silicon as linear and quadratic logistic functions of perfusion pressures with varying- intercepts and slopes by species (Supporting information X).
Model selection using Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO; @Vehtari2014; @Vehtari2017) suggested that the quadratic model performed better (the difference in expected log pointwise predictive accuracies between the two models was 207.9), and thus we only report the result of the quadratic model.


## Stan code

```{stan, file=here::here('stan/hierarchical_logistic.stan'), echo=TRUE, eval=FALSE, output.var="hoge"}
```

# Coefficients *a* and *b* without traits

## Results

The power-law fittings without (i.e., traditional approaches) and with (i.e., multilevel approaches) inter-segment variation yielded similar estimates for coefficients *a* but slightly different estimates for coefficients *b* (Figs. S5-6; Table S4).
Generally, the estimates of coefficients *a* and *b* tended to increase with increasing maximum applied pressure gradients (Fig. S7-9).
Note that the larger the maximum applied pressure gradients, the larger the sample sizes for the analyses.
Our multilevel model revealed little variance in the estimates of coefficient *a* and *b* between species (Fig. 3 and Table S5).
Xylem types (80.77% of total variance) were the largest source for the variance for coefficient *a*, while segments (40.26% of total variance) were the largest source for the variance for coefficient *b* (Table S5).


## Pool vs multi

### `r s_table("ab_comparison")`

Bayesian estimates of posterior median and 95% credible intervals of the coefficients *a* and *b* values for 31 species obtained based on traditional curve fitting (Granier, 1985) and our multilevel model.

```{r, echo=FALSE}
read_csv(here("data/all_ab.csv")) |>
  dplyr::select(
    Species = species,
    `Xylem types` = xylem_long_fct,
    pool_a = pool_full_a,
    pool_b = pool_full_b,
    multi_a = multi_full_a,
    multi_b = multi_full_b
  ) |>
  kbl() |>
  column_spec(1, italic = TRUE) |>
  kable_classic() |>
  add_header_above(c(" " = 2, "Traditional fitting" = 2, "Multilevel model" = 2))
```

### `r s_fig("pool_multi")`

Relationships between sap flux density and sap flow index K ((△T~max~-△T)/△T) for diffuse-porous tree (red), ring-porous tree (purple), palm (blue) and liana (green) species.
The K values are the averages of two sets of sensors from each segment to account for sapwood variation along the segment.
The dashed and solid lines represent posterior medians of the power-law relationships without (i.e., traditional approaches) and with (i.e., multilevel approaches) inter-segment variation, respectively.

![pool_multi](`r here::here("figs/pool_multi.png")`)


### `r s_fig("ab_points")`

Estimates of coefficients *a* (A) and *b* (B) based on traditional fitting (Granier, 1985) vs. our multilevel model.
Each point represents the posterior medians of the coefficients for each species.
Error bars show 95% credible intervals.
Dashed lines indicate 1:1 lines.

![ab_comp](`r here::here("figs/ab_points.png")`)

## Pressure gradients


### `r s_fig("pg_multi")`

Relationships between sap flux density and sap flow index K ((△T~max~-△T)/△T) for different upper limits of pressure gradients (*P~g~*) for diffuse-porous tree (red), ring-porous tree (purple), palm (blue), and liana (green) species.
Different lines indicate different maximum pressure gradients for subsets of data used to draw posterior medians of the power-law relationships with inter-segment variation (i.e., multilevel approaches).

![pg_multi](`r here::here("figs/pg_multi.png")`)

### `r s_fig("pg_ribbon_a")`

Relationships between the coefficient *a* and the maximum applied pressure gradient (*P~g~*) for diffuse-porous tree (red), ring-porous tree (purple), palm (blue), and liana (green) species.
Solid points indicate posterior medians of the power-law relationships with inter-segment variation (i.e., multilevel approaches).
Light and dark shaded areas indicate 95% and 50% Bayesian credible intervals, respectively.
The maximum applied *P~g~* are different among species because segment length are different.
Species × maximum applied *P~g~* combinations with less than 5 data points were removed from this visualization.
Therefore, the lower ranges for maximum applied *P~g~* are also different among species.

![pg_ribbon_a](`r here::here("figs/pg_ribbon_a.png")`)

### `r s_fig("pg_ribbon_b")`

Relationships between the coefficient *b* and the maximum applied pressure gradient (*P~g~*) for diffuse-porous tree (red), ring-porous tree (purple), palm (blue), and liana (green) species.
Details as for Fig. `r s_fig_num("pg_ribbon_a")`.

![pg_ribbon_b](`r here::here("figs/pg_ribbon_b.png")`)

### `r s_table("without_traits_table_0.02")`

Note: This table is too large and should be put in a separate file.
We have this table for each maximum applied *P~g~* (0.02 - 0.08).

Bayesian estimates of posterior median, upper and lower credible intervals and effective sample sizes for multilevel model without traits when we limited the maximum applied pressure gradients below 0.02 MPa.
The column definitions are:

- variable_name: name of the variable in the stan model
- level: overall, xylem type, species, or segment
- targets: name of xylem type, species, or segment
- variable_meaning: meaning of the variable
- q50: posterior median
- q2.5: 2.5% posterior quantiles
- q97.5: 97.5% posterior quantiles
- effective_sample_size: tail effective sample size > 400 indicates good convergence [@Vehtari2021].

For example, variable `beta[2,1]` indicates the coefficient *a* for diffuse-porous trees.

```{r}
withr::with_dir(rprojroot::find_root('_targets.R'),
targets::tar_read(without_traits_table_without_traits_fit_ab_summary_granier_without_traits_full_segments_sap_all_clean_0.02_data.without_traits_segments_0.02.csv)) |>
  DT::datatable()
```

### `r s_table("without_traits_table_0.08")`

Bayesian estimates of posterior median, upper and lower credible intervals and effective sample sizes for multilevel model without traits when we limited the maximum applied pressure gradients below 0.08 MPa.

```{r}
withr::with_dir(rprojroot::find_root('_targets.R'),
targets::tar_read(without_traits_table_without_traits_fit_ab_summary_granier_without_traits_full_segments_sap_all_clean_0.08_data.without_traits_segments_0.08.csv)) |>
  DT::datatable()
```


## Variance partitioning

### `r fig("coef_density")`

![coef_density](`r here::here("figs/coef_density.png")`)

Posterior distributions of coefficients *a* (A) and *b* (B) estimated by the Bayesian multilevel model that includes inter-segment, interspecific, and xylem-type variation.
Estimates for xylem type level and species level are shown.
Different colorsj indicate different xylem types.
Vertical lines indicate Granier's estimates (*a* = 119, *b* = 1.23).

### `r s_table("varpart_notrait")`

Posterior medians and 50% credible intervals of the explained variance for the coefficient *a* and *b* across three sample levels.

```{r, echo=FALSE}
read_csv(here("data/varpart_notrait.csv")) |>
  rename("Coefficient <i>a</i>" = a) |>
  rename("Coefficient <i>b</i>" = b) |>
  kbl(escape = FALSE) |>
  kable_classic()
```

# Coefficients *a* and *b* with traits

## Results

The 95% CI and 50% CI of the relationships between K~s~ and coefficient *a* and between K~s~ and coefficient *b* did not overlap with zero, respectively (Figs. 4B, D).
VAF did not show significant correlation either with coefficient *a* or coefficient *b* (Figs. 4A, C).

### `r fig("traits_points_main")`

Relationships between coefficients *a* and *b* and vessel area fraction (VAF, A, C), sapwood specific hydraulic conductivity (Ks, B, D)
Points and bars indicate posterior medians and 95% credible intervals (CIs) of coefficients for each segment.
Slopes indicate the effects of traits on the estimates of coefficients *a* and *b*.
Light and dark shaded regions indicate 95% and 50% CIs, respectively.

![traits_points_main](`r here::here("figs/traits_points_main.png")`)

### `r s_fig("traits_points_si")`

Relationships between coefficients *a* and *b* and wood density ($\rho$, A, B), hydraulic-weighted diameter (D~h~, C, D), and vessel frequency per unit area (VF, E, F).
Points and bars indicate posterior medians and 95% credible intervals (CIs) of coefficients for each segment.
None of the traits showed significant correlations with coefficients *a* or *b*.

![traits_points_si](`r here::here("figs/traits_points_si.png")`)

### `r s_table("all_seg_table")`

Note: This table is too large and should be put in a separate file.

Bayesian estimates of posterior median, upper and lower credible intervals and effective sample sizes for multilevel model with all traits.
The column definitions are:

- variable_name: name of the variable in the stan model
- level: overall, xylem type, species, or segment
- targets: name of xylem type, species, or segment
- variable_meaning: meaning of the variable
- q50: posterior median
- q2.5: 2.5% posterior quantiles
- q97.5: 97.5% posterior quantiles
- effective_sample_size: tail effective sample size > 400 indicates good convergence [@Vehtari2021].

For example, variable `beta_a[2,1]` indicates the xylem type variable and precisely the effect of wood density on coefficient *a* for diffuse-porous trees.

```{r}
withr::with_dir(rprojroot::find_root('_targets.R'),
targets::tar_read(all_seg_table)) |>
  DT::datatable()
```


```{r}
knitr::knit_exit()
```

```{r}

tar_read(without_traits_table_data.without_traits_0.035.csv)

s <- tar_read(fit_abt_summary_granier_with_traits_sap_trait_clean_all)
s2 <- tar_read(fit_abt_summary_granier_with_traits_sap_trait_clean_ks)
d <- tar_read(sap_trait_clean_all)
hist(d$xj[,3])
tar_read(all_seg_table) |>
  DT::datatable()

tar_read(ks_seg_table) |>
  DT::datatable()

s |>
  filter(str_detect(variable, "gamma")) |>
  filter(q2.5 * q97.5 > 0)
```

```{r}
d1 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(fit_abt_diagnostics_granier_with_traits_sap_trait_clean_all))
d4 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(fit_abt_diagnostics_granier_with_traits_sap_trait_clean_ks))
d6 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(fit_abt_diagnostics_granier_with_traits_sap_trait_clean_all))

apply(d1, 2, sum)
apply(d4, 2, sum)
apply(d6, 2, sum)

d4 |>
  filter(divergent__ == 1)
d6 |>
  filter(divergent__ == 1)

l1 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_all))
l2 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_dh))
l3 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_wd))
l4 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_vf))
l5 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_vaf))
l6 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_ks))
l7 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_nowd))
l8 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_novf))
l9 <- withr::with_dir(rprojroot::find_root('_targets.R'),
  targets::tar_read(traits_loo_fit_abt_mcmc_granier_with_traits_sap_trait_clean_novaf))

loo_compare(l1, l2, l3, l4, l5, l6, l7, l8, l9)
```

```{r, eval=FALSE}
devtools::session_info()
```

```{r}
library(tidyverse)
library(here)
library(lubridate)

d <- read_csv(here("data-raw/rubber_raw_data.csv")) |>
  janitor::clean_names() |>
  mutate(date = mdy_hm(date)) |>
  dplyr::select(date:time)


tmp <- read_csv(here("data-raw/rubber_raw_data.csv")) |>
  janitor::clean_names() |>
  mutate(date = mdy_hm(date)) |>
    dplyr::filter(date <= ymd("2015-03-01")) |>
    dplyr::select(-date)

apply(tmp, 2, sum, na.rm = TRUE)

d2 <- tar_read(impute_data_full)$ximp |>
  as_tibble() |>
  dplyr::select(matches("1[2-6]"))
d3 <- tar_read(combined_imputed_mapped)

d4 <- bind_cols(d, d2, d3)

d5 <- d4 |>
  head(4000) |>
  pivot_longer(t12_0_0:t11_0_0, names_to = "id", values_to = "k") |>
  mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
  mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
  mutate(dep = str_split_fixed(id, "_", 3)[, 3])

d5 |>
  filter(tree == "t01" | tree == "t02") |>
  dplyr::filter(day <= 10) |>
  ggplot(aes(x = date, y = k, col = dir, lty = tree, shape = dep)) +
  geom_point() +
  geom_line() +
  ggtitle("missing")


tmp <- names(d)[-1:-7]

d2 <- tibble(
  tree = str_split_fixed(tmp, "_", 3)[,1],
  dir = str_split_fixed(tmp, "_", 3)[,2],
  dep = str_split_fixed(tmp, "_", 3)[,3]) |>
  as.data.frame()

tar_read(impute_data_full)$ximp |> str()
tar_read(combined_imputed_mapped) |> str()






```

```{r}

d <- read_csv(here("data-raw/rubber_raw_data.csv")) |>
  janitor::clean_names() |>
  mutate(date = mdy_hm(date)) |>
  dplyr::select(date:par) |>
  dplyr::select(-vpd)

d2 <- tar_read(impute_data_full)$ximp |> as_tibble() |>
  dplyr::select(-vpd)

d3 <- tar_read(combined_imputed_mapped)

names_ <- c(names(d2), names(d3))
dup_names <- names_[duplicated(names_)]

d4 <- d2 |>
  dplyr::select(-dup_names)

imputed_df <- bind_cols(d, d4, d3) %>%
  dplyr::select(date:vpd, sort(names(.[7:45])))


dim(d)

library(tidyverse)
library(targets)
library(here)
library(lubridate)

d <- read_csv(here("data-raw/rubber_raw_data.csv")) |>
  janitor::clean_names() |>
    mutate(date = mdy_hm(date)) |>
    mutate(year = year(date)) |>
    mutate(month = month(date)) |>
    mutate(day = day(date)) |>
    mutate(yday = yday(date)) |>
    mutate(time = hour(date) * 60 + minute(date))

d <- read_csv(here("data-raw/rubber_raw_data.csv")) |>
  janitor::clean_names() |>
    mutate(date = mdy_hm(date)) |>
    mutate(year = year(date)) |>
    mutate(month = month(date)) |>
    mutate(day = day(date)) |>
    mutate(yday = yday(date)) |>
    mutate(time = hour(date) * 60 + minute(date)) |>
    mutate(tangent_transformed_day = tan((yday - 1) / 365 * 2 * pi)) |>
    mutate(tangent_transformed_time = tan(time / 1440 * 2 * pi)) |>
    dplyr::select(year, tangent_transformed_day, tangent_transformed_time,
      vpd, par, t01_0_0:t16_0_0) |>
    pivot_longer(c(t01_0_0:t16_0_0), names_to = "id", values_to = "ks")

d2 <- read_csv(here("data-raw/rubber_raw_data.csv")) |>
  janitor::clean_names() |>
    mutate(date = mdy_hm(date)) |>
    mutate(year = year(date)) |>
    mutate(month = month(date)) |>
    mutate(day = day(date)) |>
    mutate(yday = yday(date)) |>
    mutate(time = hour(date) * 60 + minute(date)) |>
    mutate(tangent_transformed_day = tan((yday - 1) / 365 * 2 * pi)) |>
    mutate(tangent_transformed_time = tan(time / 1440 * 2 * pi)) |>
    dplyr::select(year, tangent_transformed_day, tangent_transformed_time,
      vpd, par, t01_0_0:t15_0_0) |>
    pivot_longer(c(t01_0_0:t15_0_0), names_to = "id", values_to = "ks") |>
    filter(year == 2015)

date_df <- read_csv(here("data-raw/rubber_raw_data.csv")) |>
    janitor::clean_names() |>
    mutate(date = mdy_hm(date)) |>
    dplyr::select(date) |>
    dplyr::filter(date <= ymd("2016-01-31")) |>
    dplyr::filter(date >= ymd("2016-01-01"))

tar_load(imputed_df)
tar_load(imputed_all_df)

imputed_all_df$ximp |>
  as_tibble()

imputed_long_df <- bind_cols(date_df, imputed_all_df$ximp) |>
# imputed_long_df <- imputed_df |>
  pivot_longer(t01_0_0:t14_0_0, names_to = "id", values_to = "k")  |>
  mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
  mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
  mutate(dep = str_split_fixed(id, "_", 3)[, 3])

d_long_df <- d |>
  pivot_longer(t01_0_0:t16_0_0, names_to = "id", values_to = "k")  |>
  mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
  mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
  mutate(dep = str_split_fixed(id, "_", 3)[, 3])

d_long_df |>
  filter(tree == "t06" | tree == "t06") |>
  dplyr::filter(date <= ymd("2016-01-10")) |>
  dplyr::filter(date > ymd("2016-01-01")) |>
  ggplot(aes(x = date, y = k, col = dep, lty = tree, shape = dep)) +
  geom_point() +
  geom_line() +
  ggtitle("missing")

imputed_long_df |>
  filter(tree == "t06" | tree == "t06") |>
  dplyr::filter(date <= ymd("2016-01-10")) |>
  dplyr::filter(date >= ymd("2016-01-01")) |>
  ggplot(aes(x = date, y = k, col = dep, lty = tree, shape = dep)) +
  geom_point() +
  geom_line() +
  ggtitle("imputed")

d_long_df |>
  filter(tree == "t06" | tree == "t06") |>
  dplyr::filter(date <= ymd("2015-07-10")) |>
  dplyr::filter(date > ymd("2015-07-01")) |>
  ggplot(aes(x = date, y = k, col = dep, lty = tree, shape = dep)) +
  geom_point() +
  geom_line() +
  ggtitle("missing")

imputed_long_df |>
  filter(tree == "t06" | tree == "t06") |>
  dplyr::filter(date <= ymd("2015-07-10")) |>
  dplyr::filter(date > ymd("2015-07-01")) |>
  ggplot(aes(x = date, y = k, col = dep, lty = tree, shape = dep)) +
  geom_point() +
  geom_line() +
  ggtitle("imputed")


hist(rnorm(100))

ggplot(imputed_long_df, aes(x = dir, y = k)) +
  geom_boxplot() +
  facet_wrap(~tree) +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("direction imputed")

ggplot(imputed_long_df, aes(x = dep, y = k)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("depth")

imputed_long_df |>
  group_by(tree, dir, dep) |>
  summarize(k = sum(k))
```


```{r}
# Load required packages
library(tibble)
library(missForest)
library(dplyr)
library(ggplot2)

# Set random seed for reproducibility
set.seed(42)

# Generate a dummy dataset
n <- 100
group <- factor(rep(c("A", "B"), each = n))
time <- rep(1:n, 2)
x <- rnorm(2 * n)
y <- c(2 * x[1:n] + 0.5 * time[1:n] + rnorm(n), 5 * x[(n + 1):(2 * n)] + 0.5 * time[(n + 1):(2 * n)] + rnorm(n))

# Introduce missing values in y
missing_index <- sample(1:(2 * n), size = 40)
y[missing_index] <- NA

# Create the tibble
dummy_data <- tibble(group, time, x, y) |>
  as.data.frame()

# Impute missing values using missForest
imputed_data <- missForest(dummy_data)

# Extract the imputed dataset
dummy_data_imputed <- imputed_data$ximp

# Visualize the original and imputed data
dummy_data %>%
  add_row(dummy_data_imputed[missing_index, ]) %>%
  mutate(Imputed = if_else(is.na(y), "Imputed", "Original")) %>%
  ggplot(aes(x = x, y = y, color = group, shape = Imputed)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Original and Imputed Data", x = "X", y = "Y") +
  scale_shape_manual(values = c(16, 17))

```


```{r}
d <- tar_read(imputed_all_list)$ximp |>
  as_tibble()

d_long_df <- d |>
  pivot_longer(t01_0_0:t16_0_0, names_to = "id", values_to = "k")
tar_read(ks_pred_data)
fit <- read_csv("data/without_traits_segments_0.08.csv")


post <- tar_read(fit_ab_draws_granier_without_traits_full_segments_sap_all_clean_0.08) |>
  janitor::clean_names() |>
  dplyr::select(alpha_1_15, alpha_2_15)

d2 <- d_long_df |>
  group_by(id) |>
  summarize(k = sum(k)) |>
  ungroup() |>
  mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
  mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
  mutate(dep = str_split_fixed(id, "_", 3)[, 3])

# fit |>
#   # filter(str_detect(para, "alpha")) |>
#   filter(str_detect(para, "brasiliensis"))

# nd <- post |>
#   mutate(iter = 1:nrow(post)) |>
#   group_by(iter) |>
#   nest()

d3 <- d2 |>
  mutate(fd = map(k, \(k)(post$alpha_1_15  + log(k) * post$alpha_2_15))) |>
  unnest(cols = c(fd)) |>
  group_by(id, tree, dir, dep) |>
  summarize(
    mid = median(fd),
    ll = quantile(fd, 0.025),
    uu = quantile(fd, 0.975))
```

Please update the code so that each tree * diretion * depth combination has the same time series length. I.e., if the 1st combination has 100 observations, from time 1 to time 100, the other combinations should also have 100 observations from time 1 to time 100.

then all other combinations should have 100 observations. You can do this by adding a new column to the data frame that contains the time series length for each combination. Then, you can use the `tidyr::fill()` function to fill in the missing values. You can then use the `tidyr::spread()` function to convert the data frame from long to wide format. Finally, you can use the `missForest::missForest()` function to impute the missing values. Here is an example:

```{r}



```{r}

Based on the output NRMSE and PCC values, how can I judge if the model esimated missing value well?

The above code uses tidy data structure (i.e., longer format). What if I used wider format? Does this affect the estimates of missForest?

```{r}
# Load required libraries
# Create a sample dataset
set.seed(123)
n <- 100
tree <- factor(rep(1:5, each = 4 * 3 * n))
direction <- factor(rep(1:4, each = 3 * n, times = 5))
depth <- factor(rep(1:3, times = 4 * 5 * n))
k <- rnorm(5 * 4 * 3 * n, mean = 50, sd = 10)

# Add temporal pattern to k
time <- rep(1:n, times = 5 * 4 * 3)
k <- k + 5 * sin(2 * pi * time / 20)

# Add group specific offsets to k
k <- k + as.numeric(tree) * 2 + as.numeric(direction) * 1.5 + as.numeric(depth) * 0.5

# Introduce missing values
set.seed(456)
missing_idx <- sample(1:(5 * 4 * 3 * n), size = 500, replace = FALSE)
k[missing_idx] <- NA

# Combine variables into a data frame
data <- data.frame(tree = tree, direction = direction, depth = depth, k = k, time = time)

data |>
  mutate(id = interaction(tree, direction, depth)) |>
  pivot_wider(names_from = id, values_from = k)

data_wide <- data %>%
  unite("group", c("tree", "direction", "depth"), remove = FALSE) %>%
  spread(key = "group", value = "k")

# Validation metrics
nrmse <- imputed_data$OOBerror[1]
pcc <- imputed_data$OOBerror[2]

cat("Normalized Root Mean Squared Error (NRMSE):", nrmse, "\n")
cat("Proportion of Correctly Imputed Classifications (PCC):", pcc, "\n")

# Perform data imputation using missForest
set.seed(789)
imputed_data <- missForest(data)

# Extract the imputed dataset
data_imputed <- imputed_data$ximp

# Original data plot
p_original <- data |>
  filter(tree == 1 & depth == 1) |>
  ggplot(aes(x = time, y = k, color = interaction(tree, direction, depth))) +
  # geom_point() +
  geom_line() +
  theme_minimal() +
  ggtitle("Original Data") +
  xlab("Time") +
  ylab("k")
print(p_original)

# Imputed data plot
p_imputed <- ggplot(data_imputed, aes(x = time, y = k, color = interaction(tree, direction, depth))) +
  geom_point() +
  geom_line() +
  theme_minimal() +
  ggtitle("Imputed Data") +
  xlab("Time") +
  ylab("k")

print(p_imputed)

hm(1000)

head(d)

How can I make colmuns for month, day, and time for the following ?
d <- tibble(date = c("1/1/15 0:00", "1/1/15 0:10")) |>
  mutate(date = mdy_hm(date))

d |>
 mutate(month = month(date),
         day = day(date),
         time = format(date, "%H:%M"))

```

Can you covert the day starting from January 1 to ending December 31 to a single column of day of the year?

When I impute missing values using missForest, is it a good idea to use the tangent-transformed day_of_year?

```{r}
tar_read(imputed_long_list)
```

```{r}
renv::install("tarchetypes")
renv::install("languageserver")
renv::install("janitor")

library(targets)
library(tidyverse)
tmp <-
 bind_rows(tar_read(data_with_na), tar_read(combined_imputed_mapped)) |>
  head()
      filter(month %in% c(9, 10, 12))

tar_read(data_with_na) |>  str()

tar_read(data_with_na) |>  summary()

missForest::missForest(tmp)

tar_read(data_with_na) |>
  mutate(yday_back = ((atan(tangent_transformed_day) / (2 * pi)) * 365) + 1) |>
  summary()

yday <- 1:365
tangent_transformed_day = tan((yday - 1) / 365 * 2 * pi)
cosine_transformed_day = cos((yday - 1) / 365 * 2 * pi)

What about time (h) of the day (0h-24h)? What kind of transformation should I apply to this variable?

tibble(yday, tangent_transformed_day, cosine_transformed_day) |>
  ggplot(aes(x = yday, y = cosine_transformed_day)) +
  geom_point()

plot(yday, tangent_transformed_day)


# Create a dataframe with hour from 0 to 24
data <- data.frame(hour = 0:1440)

# Perform the cosine and sine transformations
data <- data %>%
  mutate(
    cosine_transformed_hour = cos((hour / 1440) * 2 * pi),
    sine_transformed_hour = sin((hour / 1440) * 2 * pi)
  )

# Plot original and transformed data
ggplot(data) +
  geom_line(aes(x = hour, y = cosine_transformed_hour), color = 'blue') +
  geom_line(aes(x = hour, y = sine_transformed_hour), color = 'red') +
  labs(title = 'Cosine and Sine Transformation of Hour of the Day',
       x = 'Hour of the Day',
       y = 'Transformed Hour') +
  scale_colour_manual("",
                      breaks = c("cosine_transformed_hour", "sine_transformed_hour"),
                      values = c("blue", "red"))

summary(tmp)

hoge |>
  filter(str_detect(name, "combined"))

csv <- tar_read(rubber_raw_data_csv)

tmp <- missForest_clean(csv, year = 2016, month = 12)
summary(tmp)

d <- read_csv(csv)

hoge <- tar_manifest()

library(tidyverse)

hoge |>
  filter(str_detect(name, "imputed_data"))

values <- expand_grid(year = c(2015, 2016), month = 1:12)

Can you make short for the following R code?

values |>
  filter(year != 2016 | month != 10) |>
  filter(year != 2016 | month != 9)

values |>
  filter(!(year == 2016 & (month == 10 | month == 9)))

d1 |> summary()

df <- tar_read(imputed_full_df)

month <- 12
year <- 2016
d1 <- read_csv(csv) |>
    janitor::clean_names() |>
    mutate(date = mdy_hm(date)) |>
    mutate(year = year(date)) |>
    mutate(month = month(date)) |>
    # filter(month == {{month}}) |>
    mutate(day = day(date)) |>
    mutate(yday = yday(date)) |>
    mutate(time = hour(date) * 60 + minute(date)) |>
    mutate(tangent_transformed_day = tan((yday - 1) / 365 * 2 * pi)) |>
    mutate(tangent_transformed_time = tan(time / 1440 * 2 * pi)) |>
    dplyr::select(year, tangent_transformed_day, tangent_transformed_time,
      vpd, par, t01_0_0:t15_0_0) |>
    pivot_longer(c(t01_0_0:t15_0_0), names_to = "id", values_to = "ks") |>
    mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
    mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
    mutate(dep = str_split_fixed(id, "_", 3)[, 3]) |>
    mutate(dir = case_when(
      dir == "0" ~ "S",
      dir == "1" ~ "E",
      dir == "2" ~ "N",
      dir == "3" ~ "W"
    )) |>
    mutate(dep = case_when(
      dep == "0" ~ 2,
      dep == "1" ~ 4,
      dep == "2" ~ 6,
    )) |>
    mutate(tree = as.factor(tree)) |>
    mutate(dir = as.factor(dir)) |>
    dplyr::select(-id) |>
    filter(year == {{year}})

d1$ks

month <- 2
d2 <- read_csv(csv) |>
    janitor::clean_names() |>
    mutate(date = mdy_hm(date)) |>
    mutate(year = year(date)) |>
    filter(year == 2015) |>
    mutate(month = month(date)) |>
    filter(month == {{month}}) |>
    mutate(day = day(date)) |>
    mutate(yday = yday(date)) |>
    mutate(time = hour(date) * 60 + minute(date)) |>
    mutate(tangent_transformed_day = tan((yday - 1) / 365 * 2 * pi)) |>
    mutate(tangent_transformed_time = tan(time / 1440 * 2 * pi)) |>
    dplyr::select(year, tangent_transformed_day, tangent_transformed_time,
      vpd, par, t01_0_0:t15_0_0) |>
    pivot_longer(c(t01_0_0:t15_0_0), names_to = "id", values_to = "ks") |>
    mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
    mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
    mutate(dep = str_split_fixed(id, "_", 3)[, 3]) |>
    mutate(dir = case_when(
      dir == "0" ~ "S",
      dir == "1" ~ "E",
      dir == "2" ~ "N",
      dir == "3" ~ "W"
    )) |>
    mutate(dep = case_when(
      dep == "0" ~ 2,
      dep == "1" ~ 4,
      dep == "2" ~ 6,
    )) |>
    mutate(tree = as.factor(tree)) |>
    mutate(dir = as.factor(dir)) |>
    dplyr::select(-id)

month <- 3

d3 <- read_csv(csv) |>
    janitor::clean_names() |>
    mutate(date = mdy_hm(date)) |>
    mutate(year = year(date)) |>
    filter(year == 2015) |>
    mutate(month = month(date)) |>
    filter(month == {{month}}) |>
    mutate(day = day(date)) |>
    mutate(yday = yday(date)) |>
    mutate(time = hour(date) * 60 + minute(date)) |>
    mutate(tangent_transformed_day = tan((yday - 1) / 365 * 2 * pi)) |>
    mutate(tangent_transformed_time = tan(time / 1440 * 2 * pi)) |>
    dplyr::select(year, tangent_transformed_day, tangent_transformed_time,
      vpd, par, t01_0_0:t15_0_0) |>
    pivot_longer(c(t01_0_0:t15_0_0), names_to = "id", values_to = "ks") |>
    mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
    mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
    mutate(dep = str_split_fixed(id, "_", 3)[, 3]) |>
    mutate(dir = case_when(
      dir == "0" ~ "S",
      dir == "1" ~ "E",
      dir == "2" ~ "N",
      dir == "3" ~ "W"
    )) |>
    mutate(dep = case_when(
      dep == "0" ~ 2,
      dep == "1" ~ 4,
      dep == "2" ~ 6,
    )) |>
    mutate(tree = as.factor(tree)) |>
    mutate(dir = as.factor(dir)) |>
    dplyr::select(-id)

apply(d1, 2, is.na) |>
  colSums()
apply(d2, 2, is.na) |>
  colSums()
apply(d3, 2, is.na) |>
  colSums()

dim(d1)
dim(d2)
dim(d3)

tar_read(data_with_na) |> str()
hist(tar_read(data_with_na)$cos_transformed_day)

tmp <- tar_read(data_with_na)$cos_transformed_day
yday = (acos(tmp) / (2 * pi)) * 365 + 1
yday |> unique()

hist(365 - unique(yday))

plot(1:92, unique(tmp))
unique(tmp) |> hist()

back_transform_day <- function(cos_transformed_day) {
  yday <- ifelse(cos_transformed_day >= 0,
                 (acos(cos_transformed_day) / (2 * pi)) * 365 + 1,
                 (2 * pi - acos(-cos_transformed_day) / (2 * pi)) * 365 + 1)
  return(yday)
}

back_transform_day <- function(cos_transformed_day) {
  yday <- ifelse(cos_transformed_day >= 0,
                 (acos(cos_transformed_day) / (2 * pi)) * 365 + 1,
                 (2 - acos(-cos_transformed_day) / (2 * pi)) * 365 + 1)
  return(yday)
}
tmp <- tar_read(data_with_na) |>
  mutate(yday2 = back_transform_day(cos_transformed_day))

tmp2 <- tmp |>
  dplyr::select(yday, yday2) |>
  unique()

rownames(tmp2) <- NULL

yday <- 1:365
# Assuming 'yday' is a numeric vector representing the day of the year
cos_transformed_day = cos((yday - 1) / 365 * 2 * pi)
sin_transformed_day = sin((yday - 1) / 365 * 2 * pi)

# To back-transform
back_transformed_day = (atan2(sin_transformed_day, cos_transformed_day) / (2 * pi)) * 365 + 1

# atan2 returns a value in the interval [-pi, pi], so we need to adjust negative values
back_transformed_day[back_transformed_day < 0] = back_transformed_day[back_transformed_day < 0] + 365


acos(1)

cos_transformed_day = cos((yday - 1) / 365 * 2 * pi)

hist(yday)

tar_read(combined_imputed_mapped) |> str()

tar_read(imputed_df_2015_9)

impute_rest_mapped <- tar_map(
  values = values2,
  tar_target(
    imputed_rest,
    {
      tmp <- missForest_clean(
        csv = rubber_raw_data_csv,
        year = year,
        month = month)
      bind_rows(tmp, df_name) |>
        missForest(parallelize = "forests")
    }
  )
)

hoge |>
  filter(str_detect(name, "imputed_rest"))

 csv <- tar_read(rubber_raw_data_csv)

d <- read_csv(csv) |>
  janitor::clean_names() |>
  mutate(date = mdy_hm(date)) |>
  mutate(year = year(date)) |>
  mutate(yday = yday(date)) |>
  mutate(time = hour(date) * 60 + minute(date)) |>
  mutate(h = time %/% 60) |>
  mutate(m = time %% 60) |>
  mutate(time = sprintf("%02d:%02d:%02d", h, m, 0)) |>
  mutate(date = ymd(paste(year, "01", "01", sep= "-")) + days(yday - 1)) |>
  dplyr::select(year, date, time,
    vpd, par, t01_0_0:t15_0_0) |>
  pivot_longer(c(t01_0_0:t15_0_0), names_to = "id", values_to = "ks") |>
  mutate(tree = str_split_fixed(id, "_", 3)[, 1]) |>
  mutate(dir = str_split_fixed(id, "_", 3)[, 2]) |>
  mutate(dep = str_split_fixed(id, "_", 3)[, 3]) |>
  mutate(dir = case_when(
    dir == "0" ~ "S",
    dir == "1" ~ "E",
    dir == "2" ~ "N",
    dir == "3" ~ "W"
  )) |>
  mutate(dep = case_when(
    dep == "0" ~ 2,
    dep == "1" ~ 4,
    dep == "2" ~ 6,
  )) |>
  mutate(tree = as.factor(tree)) |>
  mutate(dir = as.factor(dir)) |>
  dplyr::select(-id) |>
  arrange(date) |>
  arrange(dir) |>
  arrange(dep) |>
  arrange(tree)

tar_read(imputed_df_btrans_2016_1) |>
  mutate(hours = )

hoge |>
  filter(str_detect(name, "combined_imputed_rest_mapped"))

df <- tar_read(combined_imputed_mapped) |>
  bind_rows(tar_read(combined_imputed_rest_mapped)) |>
  arrange(dir) |>
  arrange(dep) |>

arrange(tree)

tar_read(imputed_full_df)

df <- tar_read(imputed_full_df)


df |>

df2 <- df |>
  mutate(id = paste(tree, dir, dep, sep = "_")) |>
  dplyr::select(-dir, -dep, -tree, -vpd, -par) |>
  pivot_wider(names_from = "id", values_from = "ks") |>
  write_csv("imputed.csv")


df2 |>
  filter(is.na(t01_N_2))

df |>
  mutate(id = paste(tree, dir, dep, sep = "_")) |>
  filter(id == "t01_N_2") |>
  filter(date == "2016-11-30") |>
  filter(time == "09:30:00")

  write_csv("imputed.csv")

  summary(df)

df |>
  mutate(h = time %/% 60) |>
  mutate(m = time %% 60) |>
  mutate(time = sprintf("%02d:%02d:%02d", h, m, 0)) |>
  dplyr::select(-h, -m)






library(lubridate)


df2 <- df |>
  mutate(date = ymd(paste(year, "01", "01", sep= "-")) + days(yday - 1))


tar_read(imputed_df_2016_2)

tar_read(imputed_df2_2016_9_imputed_df_2015_9) |>
  mutate(year = as.integer(year)) |>
        dplyr::filter(year == 2015)

tar_read(imputed_df2_2016_9_imputed_df_2015_9) |> str()
tar_read(imputed_df2_2016_9_imputed_df_2015_9) |> str()

tar_read(imputed_df_btrans2_2016_10_imputed_df_2015_10)

tar_read(imputed_df2_2016_9_imputed_df_2015_9) |>
  as.data.frame() |> head()

|>
  filter(year == 2016)

tar_read(imputed_df2_2016_12_imputed_df_2015_12)

df_9 <- tar_read(imputed_rest_2016_9_imputed_df_2015_9)$ximp |>
  as_tibble() |>
  filter(year == 2016)

tmp <- tar_read(imputed_df_btrans2_2016_10_imputed_df_2015_10)
backtransform_date(tar_read(rubber_raw_data_csv), 2016, 10, tmp)

hoge |>
  filter(str_detect(name, "btrans2"))

tar_read(imputed_rest_2016_9_imputed_df_2015_9)


hoge |>
  filter(str_detect(name, "imputed_df")) |> pull(name)

tar_load(rubber_raw_data_csv)

tar_read(imputed_df_2016_1)

missForest_clean2(rubber_raw_data_csv, 2016, 1) |> names()

read_csv(rubber_raw_data_csv) |>
    janitor::clean_names() |>
    mutate(date = mdy_hm(date))

tmp_time <- get_date(rubber_raw_data_csv, 2016, 1)
tar_read(impute_df_new_2016_8)

tmp_ori |>
  rename(yday = cos_transformed_day) |>
  rename(time = cos_transformed_time) |>
  mutate(yday = tmp_time$yday) |>
  mutate(time = tmp_time$time)



```
